# Ketama Hashing

This repository contains implementations of the Ketama Hashing algorithm in Go and C.

## Algorithm Introduction

Ketama Hashing is a specific implementation of consistent hashing, famously used in Memcached. It's a variation of the traditional hash ring approach, but with a precise method for distributing virtual nodes and using MD5 as the hashing function.

**How it works:**

1.  **MD5 Hashing:** Ketama uses the MD5 hash function to map both server names and keys to points on the hash ring.
2.  **Virtual Node Distribution:** For each physical server, multiple virtual nodes are placed on the ring. A common practice is to generate 160 virtual nodes per server (because MD5 produces a 128-bit hash, which can be interpreted as 4 32-bit integers, and 40 such points are generated per server, resulting in 40 * 4 = 160 virtual nodes). These virtual nodes are generated by hashing combinations of the server name and an index (e.g., `MD5(server_name + i)`).
3.  **Key Mapping:** To map a key, its MD5 hash is calculated. The key is then assigned to the first virtual node encountered when moving clockwise from the key's position on the ring.

**Key Features:**

*   **Good Distribution:** The large number of virtual nodes (typically 160 per server) ensures a very even distribution of keys across servers.
*   **Minimal Reassignments:** Similar to other consistent hashing algorithms, it minimizes the number of keys that need to be remapped when servers are added or removed.
*   **Widely Used:** Proven in production systems like Memcached.

## Go Implementation

### Files:

*   `go/ketama.go`: Contains the `KetamaRing` struct and its methods.
*   `go/ketama_test.go`: Contains basic tests to demonstrate the algorithm's behavior, including observing key distribution changes when nodes are added or removed.

### How to Run Tests:

Navigate to the `go` directory and run the tests:

```bash
cd go
go test -v
```

**Expected Output:**
The test output will show the distribution of keys across nodes before and after adding/removing nodes, demonstrating the consistent remapping behavior.

## C Implementation

### Files:

*   `c/ketama.h`: Header file for the Ketama hash ring functions.
*   `c/ketama.c`: Source file containing the Ketama hash ring implementation (with a simplified MD5 placeholder).
*   `c/test_ketama.c`: Main file with test cases to demonstrate node addition/removal and key re-distribution.
*   `c/Makefile`: Makefile for compiling the C code.

### How to Compile and Run Tests:

Navigate to the `c` directory, compile, and then run the executable:

```bash
cd c
make
./test_ketama
```

**Expected Output:**
Similar to the Go version, the C test output will display key distributions and show the impact of server changes.

## Experiment Results

### Key Distribution (Peak-to-Average Ratio)

This metric indicates how evenly keys are distributed across the available nodes. A ratio closer to 1.00 signifies a more even distribution.

*   **C Implementation:** 10.00 (Note: This high ratio indicates a very uneven distribution in the current C implementation, likely due to its simplified MD5 placeholder or virtual node management.)
*   **Go Implementation:** 1.12

The Go implementation shows good key distribution. The C implementation, however, exhibits a highly uneven distribution.

### Storage Overhead

This refers to the memory footprint required by the hashing algorithm's data structures.

*   **C Implementation:** The `KetamaRing` struct contains `points` (an array of `KetamaPoint` structs, each storing a virtual node hash and its corresponding node name). The overhead is proportional to the total number of virtual nodes (`NUM_VIRTUAL_NODES * NUM_NODES`).
*   **Go Implementation:** The `KetamaRing` struct contains `points` (a slice of `KetamaPoint` structs, each storing a virtual node hash and its corresponding node name). The overhead is proportional to the total number of virtual nodes (`numVirtualNodes * numNodes`).

Ketama Hashing's storage overhead is proportional to the total number of virtual nodes, which is typically higher than simpler hashing algorithms but contributes to better distribution and remapping properties.

### Impact of Node Addition/Removal on Key Remapping

This measures the percentage of keys that change their assigned node when a node is added or removed. Lower percentages indicate better consistency.

| Operation | Remapped Keys (C) | Remapped Keys (Go) | Total Keys | Percentage Remapped (C) | Percentage Remapped (Go) |
|-----------|-------------------|--------------------|------------|-------------------------|--------------------------|
| Remove    | 1101              | 1101               | 10000      | 11.01%                  | 11.01%                   |
| Add       | 0                 | 0                  | 10000      | 0.00%                   | 0.00%                    |

Ketama Hashing demonstrates good consistency, with a relatively low percentage of keys remapped (around 11%) when a node is removed. This is a key benefit of consistent hashing. The 0% remapping on adding a node back is expected, as new nodes are added to the ring without disrupting existing mappings unless a key's position now falls to the newly added node.

### Scheduling Order Result Table

Ketama Hashing uses a hash ring where keys are mapped to the first virtual node encountered when moving clockwise from the key's hash position. The "scheduling order" is implicitly defined by the sorted order of virtual nodes on the hash ring. For a given key, the algorithm performs a binary search to find its position and then selects the next virtual node in the sorted list (wrapping around if necessary).